{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset as Dataset\n",
    "from torchvision import transforms\n",
    "import multiprocessing\n",
    "import os\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms import ToPILImage\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MNISTDataSet(Dataset):\n",
    "    '''\n",
    "    path2file - string - path of the MNIST dataset, must be .pt file\n",
    "    transform - torchvision.transforms - synonymous with handler, what does this do\n",
    "    '''\n",
    "    def __init__(self, path2file, transform_handler=None):\n",
    "        self.data_path = path2file\n",
    "        self.transform_handler = transform_handler\n",
    "        self.data_model = torch.load(self.data_path)\n",
    "        # Extract and save the information in the class\n",
    "        \n",
    "    \"\"\"\n",
    "    index - index of image in dataset, no bounds checking done\n",
    "    Returns tuple of (image, number), where image is a 2D tensor, and number is a 0-dim tensor\n",
    "    \"\"\"\n",
    "    def __getitem__(self, index):\n",
    "        tens = self.data_model[0][index]\n",
    "        tens = tens.float() # To float tensor\n",
    "        tens = tens / 255.0 # Normalize\n",
    "        if (self.transform_handler != None):\n",
    "            \"\"\"\n",
    "            tens = self.data_model[0][index]\n",
    "            tens = tens.float()\n",
    "            print(\"size of tens: \", tens.size())\n",
    "            tens = tens.unsqueeze(-1) # Make tensor 3D, since ToPILImage complains\n",
    "            print(\"size unsqueezed: \", tens.size())\n",
    "            \n",
    "            import IPython;IPython.embed()\n",
    "            \n",
    "            transform = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                # normalize\n",
    "                # transforms.Normalize(),\n",
    "                #self.transform_handler,\n",
    "                transforms.Resize(10),\n",
    "                transforms.ToTensor()\n",
    "            ])\n",
    "            b = transform(tens)\n",
    "            b = tens.squeeze(-1)\n",
    "            return (b, self.data_model[1][index])\n",
    "            \"\"\"\n",
    "            tens = self.transform_handler(tens)\n",
    "        else:\n",
    "            return (tens, self.data_model[1][index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_model[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef show_mono_img(tens):\\n    if (type(tens) != torch.Tensor):\\n        raise Exception(\"Input argument should be torch.Tensor type\")\\n    img = tens.numpy()\\n    \\n    plt.figure(figsize=(1,1))\\n    plt.rcParams[\"axes.edgecolor\"] = \"black\"\\n    plt.rcParams[\"axes.linewidth\"] = 1\\n    plt.xticks([], [])\\n    plt.yticks([], [])\\n    \\n    plt.imshow(img, cmap=\\'Greys\\', interpolation=\\'none\\')\\n    \\n    plt.show()\\n\\n# DEBUGGING, COMMENT OUT TO TEST\\n# Get input data set file\\ninp_file = os.getcwd()\\ninp_file = inp_file + \"/data/processed/training.pt\"\\nprint(inp_file, type(inp_file))\\n\\n# Instantiate dataset object\\n\\nds = MNISTDataSet(inp_file, None)  # Without transformation\\n#ds = MNISTDataSet(inp_file, transforms.Resize(5)) # With a transformation\\n\\nprint(\"model type: \", type(ds.data_model))\\n#for i in range(len(ds.data_model)):\\n#    print(\"Model tuple index \", i, \" is of type: \", type(ds.data_model[i]), \", example: \", ds.data_model[i][0])\\nprint(\"num pics: \", len(ds))\\n#print(ds.data_model)\\n\\nfor i in range(0, 5):\\n    img_tensor, class_tensor = ds[i]\\n    #print(\"type of image: \", type(ds[i]))\\n    #print(\"dim of image: \", ds[i].size())        \\n    show_mono_img(img_tensor)\\n    print(class_tensor.item())\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Displays a tensor that represents a monocrome image\n",
    "'''\n",
    "'''\n",
    "def show_mono_img(tens):\n",
    "    if (type(tens) != torch.Tensor):\n",
    "        raise Exception(\"Input argument should be torch.Tensor type\")\n",
    "    img = tens.numpy()\n",
    "    \n",
    "    plt.figure(figsize=(1,1))\n",
    "    plt.rcParams[\"axes.edgecolor\"] = \"black\"\n",
    "    plt.rcParams[\"axes.linewidth\"] = 1\n",
    "    plt.xticks([], [])\n",
    "    plt.yticks([], [])\n",
    "    \n",
    "    plt.imshow(img, cmap='Greys', interpolation='none')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# DEBUGGING, COMMENT OUT TO TEST\n",
    "# Get input data set file\n",
    "inp_file = os.getcwd()\n",
    "inp_file = inp_file + \"/data/processed/training.pt\"\n",
    "print(inp_file, type(inp_file))\n",
    "\n",
    "# Instantiate dataset object\n",
    "\n",
    "ds = MNISTDataSet(inp_file, None)  # Without transformation\n",
    "#ds = MNISTDataSet(inp_file, transforms.Resize(5)) # With a transformation\n",
    "\n",
    "print(\"model type: \", type(ds.data_model))\n",
    "#for i in range(len(ds.data_model)):\n",
    "#    print(\"Model tuple index \", i, \" is of type: \", type(ds.data_model[i]), \", example: \", ds.data_model[i][0])\n",
    "print(\"num pics: \", len(ds))\n",
    "#print(ds.data_model)\n",
    "\n",
    "for i in range(0, 5):\n",
    "    img_tensor, class_tensor = ds[i]\n",
    "    #print(\"type of image: \", type(ds[i]))\n",
    "    #print(\"dim of image: \", ds[i].size())        \n",
    "    show_mono_img(img_tensor)\n",
    "    print(class_tensor.item())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook dataset.ipynb to script\n",
      "[NbConvertApp] Writing 3602 bytes to dataset.py\n"
     ]
    }
   ],
   "source": [
    "# Run this to convert the juypter notebook to a python file\n",
    "!jupyter nbconvert --to script dataset.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
